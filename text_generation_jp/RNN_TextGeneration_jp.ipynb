{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1167caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9dd360",
   "metadata": {},
   "source": [
    "## 讀取數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4972d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./I am a CAT.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640f10d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 323574 characters\n"
     ]
    }
   ],
   "source": [
    "# py2compat読み取り\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# テキストの長さは、テキストの文字数を指します\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5047622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は猫である\r\n",
      "夏目漱石\r\n",
      "一\r\n",
      "　吾輩は猫である。名前はまだ無い。\r\n",
      "　どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかり\n"
     ]
    }
   ],
   "source": [
    "# テキストの最初の250文字を見てください\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c19bba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3058 unique characters\n"
     ]
    }
   ],
   "source": [
    "# テキスト内の繰り返しのない文字\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126dfd0c",
   "metadata": {},
   "source": [
    "# テキストの処理\n",
    "## ベクトル化されたテキスト\n",
    "トレーニングの前に、文字列を数値表現値にマップする必要があります。 2つのルックアップテーブルを作成します。1つは文字を数字にマップし、もう1つは数字を文字にマップします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1acd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繰り返されない文字からインデックスへのマッピングを作成する\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c47dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  \"'\" :   3,\n",
      "  '(' :   4,\n",
      "  ')' :   5,\n",
      "  '-' :   6,\n",
      "  '.' :   7,\n",
      "  '0' :   8,\n",
      "  '1' :   9,\n",
      "  '2' :  10,\n",
      "  '3' :  11,\n",
      "  '4' :  12,\n",
      "  '6' :  13,\n",
      "  '7' :  14,\n",
      "  '8' :  15,\n",
      "  '=' :  16,\n",
      "  'A' :  17,\n",
      "  'D' :  18,\n",
      "  'E' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 各文字には整数値があります。 文字をインデックス0からlen（unique）にマップすることに注意してください。\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b107f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'吾輩は猫である\\r\\n夏目漱石' ---- characters mapped to int ---- > [ 587 2646  103 1770   95   63  131    1    0  739 1888 1662 1925]\n"
     ]
    }
   ],
   "source": [
    "# テキストの最初の13文字の整数マッピングを表示します\n",
    "\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba7690",
   "metadata": {},
   "source": [
    "## 予測タスク\n",
    "文字または文字のシーケンスが与えられた場合、次に可能性の高い文字は何ですか？ これは、モデルをトレーニングするために実行する必要のあるタスクです。 モデルへの入力は文字のシーケンスであり、出力を予測するようにモデルをトレーニングします。各タイムステップで、次の文字がどうなるかを予測します。\n",
    "\n",
    "RNNは以前に見た要素に基づいて内部状態を維持するため、この時点で計算されたすべての文字を考えると、次の文字は何ですか？\n",
    "\n",
    "## トレーニングサンプルとターゲットを作成する\n",
    "次に、テキストをサンプルシーケンスに分割します。 各入力シーケンスには、テキストにseq_length文字が含まれています。\n",
    "\n",
    "各入力シーケンスについて、対応するターゲットには同じ長さのテキストが含まれていますが、1文字右にシフトされています。\n",
    "\n",
    "テキストを長さseq_length + 1のテキストブロックに分割します。 たとえば、seq_lengthが4で、テキストが「Hello」の場合、入力シーケンスは「Hell」になり、ターゲットシーケンスは「ello」になります。\n",
    "これを行うには、最初にtf.data.Dataset.from_tensor_slices関数を使用して、テキストベクトルを文字インデックスストリームに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75734fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾\n",
      "輩\n",
      "は\n",
      "猫\n",
      "で\n"
     ]
    }
   ],
   "source": [
    "# 各入力文の最大長を設定します\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# トレーニングサンプル/ターゲットを作成する\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8088f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'吾輩は猫である\\r\\n夏目漱石\\r\\n一\\r\\n\\u3000吾輩は猫である。名前はまだ無い。\\r\\n\\u3000どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人'\n",
      "'間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいと'\n",
      "'も思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ちついて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思'\n",
      "'った感じが今でも残っている。第一毛をもって装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にもだいぶ逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中があまりに突起している。そう'\n",
      "'してその穴の中から時々ぷうぷうと煙を吹く。どうも咽せぽくて実に弱った。これが人間の飲む煙草というものである事はようやくこの頃知った。\\r\\n\\u3000この書生の掌の裏でしばらくはよい心持に坐っておったが、しばらくす'\n"
     ]
    }
   ],
   "source": [
    "# batch では、1文字を必要な長さのシーケンスに簡単に変換できます。\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3df6edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# シーケンスごとに、mapメソッドを使用してコピーし、次に進んで入力テキストとターゲットテキストを作成します。 mapメソッドは、各バッチに単純な関数を適用できます。\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c7d2446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '吾輩は猫である\\r\\n夏目漱石\\r\\n一\\r\\n\\u3000吾輩は猫である。名前はまだ無い。\\r\\n\\u3000どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて'\n",
      "Target data: '輩は猫である\\r\\n夏目漱石\\r\\n一\\r\\n\\u3000吾輩は猫である。名前はまだ無い。\\r\\n\\u3000どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人'\n"
     ]
    }
   ],
   "source": [
    "# サンプルの最初のバッチの入力値とターゲット値を印刷します：\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47df189",
   "metadata": {},
   "source": [
    "これらのベクトルの各インデックスは、タイムステップとして処理されます。 タイムステップ0への入力として、モデルは「F」のインデックスを受け取り、次の文字として「i」のインデックスを予測しようとします。 次のタイムステップでは、モデルは同じ操作を実行しますが、RNNは現在の入力文字だけでなく、前のステップからの情報も考慮します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76b0f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 587 ('吾')\n",
      "  expected output: 2646 ('輩')\n",
      "Step    1\n",
      "  input: 2646 ('輩')\n",
      "  expected output: 103 ('は')\n",
      "Step    2\n",
      "  input: 103 ('は')\n",
      "  expected output: 1770 ('猫')\n",
      "Step    3\n",
      "  input: 1770 ('猫')\n",
      "  expected output: 95 ('で')\n",
      "Step    4\n",
      "  input: 95 ('で')\n",
      "  expected output: 63 ('あ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c1680",
   "metadata": {},
   "source": [
    "## トレーニングバッチを作成する\n",
    "以前は、tf.dataを使用して、テキストを管理可能なシーケンスに分割しました。 ただし、このデータをモデルに送信する前に、データをシャッフルしてバッチにパックする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c451dd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バッチサイズ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# データセットを再配置するためのバッファサイズを設定します\n",
    "#（TFデータは、潜在的に無制限のシーケンスを処理するように設計されています。\n",
    "# したがって、メモリ内のシーケンス全体を再配置しようとはしません。 それどころか、\n",
    "# バッファを維持し、バッファ内の要素を再配置します。 ）。\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c8611",
   "metadata": {},
   "source": [
    "## モデルを作成する\n",
    "tf.keras.Sequentialを使用してモデルを定義します。 この簡単な例では、3つのレイヤーを使用してモデルを定義しました。\n",
    "\n",
    "* tf.keras.layers.Embedding：入力レイヤー。 各文字の数をembedding_dim次元のベクトルにマップするトレーニング可能な比較テーブル。\n",
    "* tf.keras.layers.GRU：サイズがunits = rnn_unitsで指定されているRNNのタイプ（ここでLSTMレイヤーを使用することもできます）。\n",
    "* tf.keras.layers.Dense：vocab_size出力を備えた出力レイヤー。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a40618c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語セットの長さ\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 埋め込まれた寸法\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNNユニットの数\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a50981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "275a18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02629b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 3058) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64d539da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           782848    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 3058)          3134450   \n",
      "=================================================================\n",
      "Total params: 7,855,602\n",
      "Trainable params: 7,855,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a52a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d039204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1347, 2761, 2216, 2195, 3027, 1180, 1517, 1371,  992, 1746, 2447,\n",
       "       2715, 1298, 1306, 1385,  657, 2084, 1048,  120, 2335,  399, 2899,\n",
       "       1497, 1645, 2114,  149, 1005, 1838, 2833, 1794, 2404,  846,  313,\n",
       "       2514,  451, 1698,  197, 1284, 2885,  911, 1891,  341,  963, 1542,\n",
       "        987, 1346, 1387,   11, 2523, 1938,  162, 1423, 2990, 1056, 1306,\n",
       "        478, 2345,  321, 2215,  226, 2507, 2772, 1282, 2227, 2304,  413,\n",
       "        598, 1539,  769,   31,   33, 2201, 1801,  854, 1487, 2010,  902,\n",
       "       1014, 1139,  500,  579, 2252, 3020, 1485, 1753, 1866,  447, 2810,\n",
       "       3001, 1647,  766, 1870,  568, 2310, 2818, 1108, 1766, 1633, 2514,\n",
       "        691], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d206aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '、さすがに独仙君は鎌倉へ行って万年漬を食っただけあって、物に動じないね。どうも敬々服々だ。碁はまずいが、度胸は据ってる」\\r\\n「だから君のような度胸のない男は、少し真似をするがいい」と主人が後ろ向のまま'\n",
      "\n",
      "Next Char Predictions: \n",
      " '普鈴胡肉鼓拓武更弁牡西避敷斤期噂素念む蔭兜頻機滅綺ギ張疎陽琳衒宿併誹利炭ム放韻崇相倒幽水延晩朧3諷碗セ架鳴怨斤功薔侮胞与誡銷改脱菜冉呼氏如gi肢瑩寞槽童岳彷成匆君臓麻槍犯癬列闖鵯滑女白号萎陌慕狽溌誹坑'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a47713f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 3058)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       8.025424\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "078e68ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39bdfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查點保存至的目錄\n",
    "checkpoint_dir = './training_checkpoints_2'\n",
    "\n",
    "# 檢查點的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68b698c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0679372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c3d6239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 96s 2s/step - loss: 5.7043\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 98s 2s/step - loss: 4.9302\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 99s 2s/step - loss: 4.3893\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 102s 2s/step - loss: 4.1184\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 99s 2s/step - loss: 3.9479\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 101s 2s/step - loss: 3.8195\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 102s 2s/step - loss: 3.7156\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 104s 2s/step - loss: 3.6300\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 106s 2s/step - loss: 3.5521\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 99s 2s/step - loss: 3.4826\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24d954",
   "metadata": {},
   "source": [
    "# テキストを生成する\n",
    "## 最新のチェックポイントを復元する\n",
    "この予測ステップを単純に保つために、バッチサイズを1に設定します。 RNN状態がタイムステップからタイムステップに転送される方法により、モデルの構築後は固定バッチサイズのみが受け入れられます。 異なるbatch_sizeでモデルを実行するには、モデルを再構築し、チェックポイントから重みを復元する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1467af62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints_2\\\\ckpt_10'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f19388f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0397a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            782848    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 3058)           3134450   \n",
      "=================================================================\n",
      "Total params: 7,855,602\n",
      "Trainable params: 7,855,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01a242",
   "metadata": {},
   "source": [
    "# 予測サイクル\n",
    "次のコードブロックはテキストを生成します。\n",
    "\n",
    "* 最初に開始文字列を設定し、RNN状態を初期化し、生成する文字数を設定します。\n",
    "\n",
    "* 開始文字列とRNN状態を使用して、次の文字の予測分布を取得します。\n",
    "\n",
    "* 次に、分類分布を使用して、予測された文字のインデックスが計算されます。 この予測された文字をモデルへの次の入力として使用します。\n",
    "\n",
    "* モデルによって返されたRNN状態は、モデルに送り返されます。 これで、モデルには、1つの文字だけでなく、学習するコンテキストが増えました。 次の文字を予測した後、変更されたRNN状態がモデルに再度返送されます。 モデルはこのようなもので、以前に予測されたキャラクターから常により多くのコンテキストを取得することによって学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01f56875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # 評価手順（学習したモデルを使用してテキストを生成します）\n",
    "\n",
    "    # 生成される文字数\n",
    "    num_generate = 1000\n",
    "\n",
    "    # 開始文字列を数値に変換する（ベクトル化）\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # 結果を格納するために空の文字列が使用されます\n",
    "    text_generated = []\n",
    "\n",
    "    # 低温はより予測可能なテキストを生成します\n",
    "    # 温度が高くなると、より驚くべきテキストが生成されます\n",
    "    # 実験して最適な設定を見つけることができます\n",
    "    temperature = 1.0\n",
    "\n",
    "    # バッチサイズは1です。\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # バッチの寸法を削除します\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # カテゴリ分布予測モデルによって返される文字\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # 予測された文字と前の非表示状態を次の入力としてモデルに渡します\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45139e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は猫と令嬢であろう、腹で道望と猫がへえて、その辺の実をそれは道用がったら徒とこの何でも是非歌h T要も云い本したっと云う気がない。実は奥晶の消えて、長を楯にして云う。吾輩はどうも読んで、ためなくて、これは利かなる結論だ。しかし、平均の眼を通りつくなくなるの風が」\r\n",
      "「どこしいかくきで別切と云けぬ。さった。\r\n",
      "「この時はこの暁であるかれかならんと心配畠の布団の二三襦色が往来ても心を開き得て催いいの。人も純なものか」\r\n",
      "「主人甜郷灯も這入らずに妨るない」\r\n",
      "「何に尾主歌を製化して糞がなぜんじゃんでする。主人は鉄山の上でもうと思うようがは聳えなかわないっしっとはこれじゃないとらやって女子と云っている越え牧坪ば白いだろ、さんで今なにこの鏘すで行ったが、相違あってから拝見順のは論ちに毛を敲くようならべく見受呑心などなは妻にやろまいうなどほか分知れてい」\r\n",
      "「これは二ついくと歯のである。このあと今なはあるの観かなくだって近物の演桶の御曰く落ちのように上らんが、その知らずるようにむては月並と評しは人衣を入らずかろせない。球である赤地でに一取別のと行く分らない、すでもたり、四つの講ジプ上であるなものみば池を食ったとかろか。主人は世法を突然人間になる。吾輩が耳の新深湯寺の嫌いじゃないかめものだろう。しか椽側かある。\r\n",
      "「あれは不平君はたのかうか、そうだがっでしかければしょっこのと足もあと主人の犬に同じを地より度もまく存価している。\r\n",
      "「うんよ猫の二円と主人の気色の法す水草を放逐なるところが「いようそうだから分ると前から喜滑を攻撃よるともち英快そうだろうが、三分日火者などこへ感じたって愛の説劇だそうだ。寒月君で聞見たっためと象の逆からいのなものか、二客だけだたのはなのじゃあれると思う。「あいく事もない。\r\n",
      "「冗談しか、三間の人にかってハーゼ縊りなるのは世際な世のだっては舐めんよ。いってもって同毛をして癒らると泣きるから、ごとく楽不分から人勢だが流は捕え抜けりばかりが頭を借る布体なるほどいであの猫など同じた遠らは冗三四動器を隔たる。彼れらく年にやりとするも、少しのさ、僕ばかって…―無儀を、いつかろ」\r\n",
      "「しい字野さん、するのところがあるが彼等朝君ら」と主人学性を貰いたまあ逆上術ジジクの人は睨めも入れない動をしても忘れる、湯に両人、運動には一つもないか、一人が、胸の以外有して見る。気薫もない。\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"吾輩は猫\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0d37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
